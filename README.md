# Optimization-Algorithms-and-Activation-Functions-

Different Optimization Techniques were tested and visualised on a simple Sigmoid Neuron. I plotted their 2-d and 3-d plots to analyze their performance. A self-made random dataset was created. The optimization algorithms tested are : 
    
        1. Gradient Descent
        2. Stochastic Gradient Descent
        3. Momentum-based Gradient Descent
        4. Nesterov Accelerated Gradient Descent
        5. Mini-Batch 
        6. RMSProp
        7. AdamGrad
        8. Adam Optimization


Similary, different Activation functions are tested and their performance visualised on a FeedForward Neural Network. The dataset was created using make-blobs library from sklearn.datasets. The activation functions tested are:
   
        1. Sigmoid Activation Function
        2. Tanh Function
        3. ReLu Function
        4. Leaky-ReLu Function
  
 This repository also contains a jupyter notebook in which L-2 Regularization Technique is evaluated and analyzed on a FeedForward Neural Network.
